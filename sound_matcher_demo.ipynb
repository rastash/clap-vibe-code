{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLAP Sound Matcher Demo\n",
    "\n",
    "This notebook demonstrates the CLAP Sound Matcher system, which uses the CLAP (Contrastive Language-Audio Pretraining) model to match text queries to audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "import json\n",
    "import hashlib\n",
    "import IPython.display as ipd\n",
    "from msclap import CLAP\n",
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the SoundMatcher Class\n",
    "\n",
    "We'll define the SoundMatcher class, which is responsible for matching text queries to audio files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class SoundMatcher:\n",
    "    def __init__(self, audio_dir, use_cuda=False, cache_dir=\"cache\", force_recompute=False):\n",
    "        \"\"\"\n",
    "        Initialize the SoundMatcher with a directory of audio files\n",
    "        \n",
    "        Args:\n",
    "            audio_dir (str): Path to directory containing audio files\n",
    "            use_cuda (bool): Whether to use CUDA for CLAP model\n",
    "            cache_dir (str): Directory to store cached embeddings\n",
    "            force_recompute (bool): Force recomputation of embeddings even if cache exists\n",
    "        \"\"\"\n",
    "        self.audio_dir = audio_dir\n",
    "        self.device = \"cuda\" if use_cuda and torch.cuda.is_available() else \"cpu\"\n",
    "        self.cache_dir = cache_dir\n",
    "        self.force_recompute = force_recompute\n",
    "        \n",
    "        # Create cache directory if it doesn't exist\n",
    "        os.makedirs(self.cache_dir, exist_ok=True)\n",
    "        \n",
    "        # Load CLAP model\n",
    "        print(\"Loading CLAP model...\")\n",
    "        self.clap_model = CLAP(version='2023', use_cuda=use_cuda)\n",
    "        \n",
    "        # Find all audio files\n",
    "        self.audio_files = self._get_audio_files()\n",
    "        print(f\"Found {len(self.audio_files)} audio files\")\n",
    "        \n",
    "        # Pre-compute audio embeddings\n",
    "        self.audio_embeddings = None\n",
    "        self.index_to_file = {}\n",
    "        self._load_or_compute_audio_embeddings()\n",
    "    \n",
    "    def _get_audio_files(self):\n",
    "        \"\"\"Get all audio files in the directory\"\"\"\n",
    "        audio_extensions = ['.wav', '.mp3', '.ogg', '.flac']\n",
    "        audio_files = []\n",
    "        \n",
    "        for ext in audio_extensions:\n",
    "            audio_files.extend(glob.glob(os.path.join(self.audio_dir, f\"**/*{ext}\"), recursive=True))\n",
    "        \n",
    "        return sorted(audio_files)  # Sort to ensure consistent ordering\n",
    "    \n",
    "    def _get_cache_filename(self):\n",
    "        \"\"\"Generate a consistent filename for the cache based on the audio files\"\"\"\n",
    "        # Create a hash of the audio directory path\n",
    "        hasher = hashlib.md5()\n",
    "        hasher.update(self.audio_dir.encode('utf-8'))\n",
    "        \n",
    "        # Add the number of files to the hash\n",
    "        hasher.update(str(len(self.audio_files)).encode('utf-8'))\n",
    "        \n",
    "        # Generate a hash digest\n",
    "        hash_digest = hasher.hexdigest()\n",
    "        \n",
    "        return hash_digest\n",
    "    \n",
    "    def _get_cache_path(self):\n",
    "        \"\"\"Get the path to the cache file\"\"\"\n",
    "        cache_filename = self._get_cache_filename()\n",
    "        return os.path.join(self.cache_dir, f\"audio_embeddings_{cache_filename}.npz\")\n",
    "    \n",
    "    def _get_metadata_path(self):\n",
    "        \"\"\"Get the path to the metadata file\"\"\"\n",
    "        cache_filename = self._get_cache_filename()\n",
    "        return os.path.join(self.cache_dir, f\"metadata_{cache_filename}.json\")\n",
    "    \n",
    "    def _save_embeddings(self):\n",
    "        \"\"\"Save embeddings to cache file\"\"\"\n",
    "        cache_path = self._get_cache_path()\n",
    "        metadata_path = self._get_metadata_path()\n",
    "        \n",
    "        # Convert tensor to numpy for saving\n",
    "        embeddings_np = self.audio_embeddings.detach().cpu().numpy()\n",
    "        np.savez_compressed(cache_path, embeddings=embeddings_np)\n",
    "        \n",
    "        # Save mapping from index to file\n",
    "        with open(metadata_path, 'w') as f:\n",
    "            json.dump(self.index_to_file, f)\n",
    "        \n",
    "        print(f\"Saved embeddings to {cache_path}\")\n",
    "    \n",
    "    def _load_embeddings(self):\n",
    "        \"\"\"Load embeddings from cache file\"\"\"\n",
    "        cache_path = self._get_cache_path()\n",
    "        metadata_path = self._get_metadata_path()\n",
    "        \n",
    "        if os.path.exists(cache_path) and os.path.exists(metadata_path):\n",
    "            print(f\"Loading cached embeddings from {cache_path}\")\n",
    "            \n",
    "            # Load embeddings\n",
    "            data = np.load(cache_path)\n",
    "            embeddings_np = data['embeddings']\n",
    "            self.audio_embeddings = torch.tensor(embeddings_np, dtype=torch.float32)\n",
    "            \n",
    "            # Load mapping from index to file\n",
    "            with open(metadata_path, 'r') as f:\n",
    "                # JSON keys are strings, convert them back to integers\n",
    "                index_to_file_str = json.load(f)\n",
    "                self.index_to_file = {int(k): v for k, v in index_to_file_str.items()}\n",
    "            \n",
    "            return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _load_or_compute_audio_embeddings(self):\n",
    "        \"\"\"Load embeddings from cache if available, otherwise compute them\"\"\"\n",
    "        if not self.force_recompute and self._load_embeddings():\n",
    "            print(f\"Loaded embeddings with shape: {self.audio_embeddings.shape}\")\n",
    "            return\n",
    "        \n",
    "        print(\"Computing audio embeddings...\")\n",
    "        \n",
    "        # Process in batches to avoid memory issues\n",
    "        batch_size = 50\n",
    "        all_embeddings = []\n",
    "        \n",
    "        for i in tqdm(range(0, len(self.audio_files), batch_size)):\n",
    "            batch_files = self.audio_files[i:i+batch_size]\n",
    "            batch_embeddings = self.clap_model.get_audio_embeddings(batch_files)\n",
    "            \n",
    "            # Convert to PyTorch tensor if it's not already\n",
    "            if not isinstance(batch_embeddings, torch.Tensor):\n",
    "                batch_embeddings = torch.tensor(batch_embeddings, dtype=torch.float32)\n",
    "            else:\n",
    "                # Detach if it has gradients\n",
    "                batch_embeddings = batch_embeddings.detach()\n",
    "                \n",
    "            all_embeddings.append(batch_embeddings)\n",
    "            \n",
    "            # Create mapping from index to file\n",
    "            for j, file_path in enumerate(batch_files):\n",
    "                self.index_to_file[i + j] = file_path\n",
    "        \n",
    "        # Stack all embeddings into a single tensor\n",
    "        self.audio_embeddings = torch.cat(all_embeddings, dim=0)\n",
    "        print(f\"Computed embeddings with shape: {self.audio_embeddings.shape}\")\n",
    "        \n",
    "        # Save embeddings to cache\n",
    "        self._save_embeddings()\n",
    "    \n",
    "    def find_matching_sounds(self, text_query, top_k=5):\n",
    "        \"\"\"\n",
    "        Find the top-k sounds matching the text query\n",
    "        \n",
    "        Args:\n",
    "            text_query (str): Text description of the sound\n",
    "            top_k (int): Number of top matches to return\n",
    "            \n",
    "        Returns:\n",
    "            list: List of (file_path, similarity_score) tuples\n",
    "        \"\"\"\n",
    "        # Get text embedding\n",
    "        text_embedding = self.clap_model.get_text_embeddings([text_query])\n",
    "        \n",
    "        # Convert to PyTorch tensor if it's not already\n",
    "        if not isinstance(text_embedding, torch.Tensor):\n",
    "            text_embedding = torch.tensor(text_embedding, dtype=torch.float32)\n",
    "        else:\n",
    "            # Detach if it has gradients\n",
    "            text_embedding = text_embedding.detach()\n",
    "        \n",
    "        # Compute similarities\n",
    "        similarities = self.clap_model.compute_similarity(self.audio_embeddings, text_embedding)\n",
    "        \n",
    "        # Convert to numpy for easier processing if it's a tensor\n",
    "        if isinstance(similarities, torch.Tensor):\n",
    "            similarities = similarities.detach().cpu().numpy()\n",
    "        \n",
    "        # Get top-k matches\n",
    "        top_indices = np.argsort(similarities.flatten())[-top_k:][::-1]\n",
    "        \n",
    "        # Return file paths and scores\n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            file_path = self.index_to_file[idx]\n",
    "            score = similarities[idx][0]\n",
    "            results.append((file_path, score))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def visualize_audio(self, file_path):\n",
    "        \"\"\"\n",
    "        Visualize the audio waveform and spectrogram\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to audio file\n",
    "        \"\"\"\n",
    "        # Load audio file\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "        # Plot waveform and spectrogram\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        \n",
    "        # Waveform\n",
    "        plt.subplot(2, 1, 1)\n",
    "        librosa.display.waveshow(y, sr=sr)\n",
    "        plt.title('Waveform')\n",
    "        \n",
    "        # Spectrogram\n",
    "        plt.subplot(2, 1, 2)\n",
    "        D = librosa.amplitude_to_db(np.abs(librosa.stft(y)), ref=np.max)\n",
    "        librosa.display.specshow(D, sr=sr, x_axis='time', y_axis='log')\n",
    "        plt.colorbar(format='%+2.0f dB')\n",
    "        plt.title('Spectrogram')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Sound Matcher\n",
    "\n",
    "Now let's initialize the sound matcher with the ESC-50 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize sound matcher with the ESC-50 dataset\n",
    "audio_dir = 'data/ESC-50-master/audio'\n",
    "matcher = SoundMatcher(audio_dir, use_cuda=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match Text Queries to Sounds\n",
    "\n",
    "Now let's try matching some text queries to sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def search_and_play(query, top_k=3):\n",
    "    \"\"\"Search for sounds matching the query and play them\"\"\"\n",
    "    # Find matching sounds\n",
    "    matches = matcher.find_matching_sounds(query, top_k=top_k)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nTop {top_k} matches for query: '{query}'\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Display results with audio players\n",
    "    for i, (file_path, score) in enumerate(matches):\n",
    "        file_name = os.path.basename(file_path)\n",
    "        print(f\"{i+1}. {file_name} (score: {score:.4f})\")\n",
    "        \n",
    "        # Display audio player\n",
    "        display(ipd.Audio(file_path))\n",
    "        \n",
    "        # Visualize the audio if it's the top match\n",
    "        if i == 0:\n",
    "            print(f\"\\nVisualizing top match: {file_name}\")\n",
    "            matcher.visualize_audio(file_path)\n",
    "    \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Queries\n",
    "\n",
    "Let's try some example queries to see how well the system works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example 1: Dog barking\n",
    "search_and_play(\"dog barking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example 2: Rain falling\n",
    "search_and_play(\"rain falling\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example 3: Clock ticking\n",
    "search_and_play(\"clock ticking\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example 4: Baby crying\n",
    "search_and_play(\"baby crying\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Example 5: Car engine\n",
    "search_and_play(\"car engine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Search\n",
    "\n",
    "Let's create an interactive widget to search for sounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Create widgets\n",
    "query_input = widgets.Text(value='', description='Query:', placeholder='Enter a text query')\n",
    "top_k_slider = widgets.IntSlider(value=3, min=1, max=10, step=1, description='Top K:')\n",
    "search_button = widgets.Button(description='Search')\n",
    "output = widgets.Output()\n",
    "\n",
    "# Define search function\n",
    "def on_search_button_clicked(b):\n",
    "    with output:\n",
    "        clear_output()\n",
    "        search_and_play(query_input.value, top_k=top_k_slider.value)\n",
    "\n",
    "# Connect button to function\n",
    "search_button.on_click(on_search_button_clicked)\n",
    "\n",
    "# Display widgets\n",
    "display(widgets.VBox([query_input, top_k_slider, search_button, output]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've demonstrated how to use the CLAP model to match text queries to audio files. The system works by computing embeddings for both the text query and the audio files, and then finding the audio files with embeddings most similar to the text embedding.\n",
    "\n",
    "The system can be used for a variety of applications, such as:\n",
    "- Sound effect search engines\n",
    "- Audio dataset exploration\n",
    "- Content-based audio retrieval\n",
    "- Audio sample organization\n",
    "\n",
    "Future improvements could include:\n",
    "- Fine-tuning the CLAP model on specific audio domains\n",
    "- Adding support for audio-to-audio search\n",
    "- Implementing a web interface for easier access\n",
    "- Adding more advanced audio visualization techniques"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
